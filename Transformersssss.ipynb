{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj50T8HngT6U",
        "outputId": "9021341b-34a2-44ec-ac17-83ecb21b19c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:\n",
            " tensor([[4.3061e-01, 1.5894e-01, 1.1955e-02, 5.1078e-05, 1.9447e-01, 2.0204e-01,\n",
            "         1.9282e-03],\n",
            "        [4.3387e-01, 1.5085e-01, 1.3501e-02, 6.7036e-05, 2.1080e-01, 1.8826e-01,\n",
            "         2.6425e-03],\n",
            "        [3.7908e-01, 1.7508e-01, 2.2295e-02, 3.0595e-04, 2.0734e-01, 2.1051e-01,\n",
            "         5.3868e-03],\n",
            "        [3.0247e-01, 1.8655e-01, 5.9879e-02, 4.9815e-03, 2.1277e-01, 2.0590e-01,\n",
            "         2.7445e-02],\n",
            "        [4.3384e-01, 1.5517e-01, 1.4095e-02, 7.1856e-05, 1.9819e-01, 1.9614e-01,\n",
            "         2.4938e-03],\n",
            "        [4.1533e-01, 1.6272e-01, 1.3794e-02, 7.9935e-05, 1.9983e-01, 2.0567e-01,\n",
            "         2.5728e-03],\n",
            "        [3.4870e-01, 1.7962e-01, 3.5365e-02, 1.0997e-03, 2.1879e-01, 2.0481e-01,\n",
            "         1.1622e-02]])\n",
            "\n",
            "Self-Attention Output:\n",
            " tensor([[2.5879, 3.2746, 3.5969, 2.6043, 1.6062, 1.6859, 1.5454, 2.7297],\n",
            "        [2.5863, 3.2799, 3.5910, 2.6014, 1.6029, 1.6822, 1.5475, 2.7391],\n",
            "        [2.5700, 3.2584, 3.5961, 2.6006, 1.6002, 1.6808, 1.5254, 2.6943],\n",
            "        [2.5252, 3.2045, 3.5454, 2.5599, 1.5766, 1.6471, 1.4806, 2.6247],\n",
            "        [2.5875, 3.2755, 3.5925, 2.6017, 1.6048, 1.6837, 1.5464, 2.7336],\n",
            "        [2.5832, 3.2711, 3.5979, 2.6043, 1.6047, 1.6850, 1.5400, 2.7202],\n",
            "        [2.5542, 3.2439, 3.5819, 2.5890, 1.5919, 1.6702, 1.5106, 2.6728]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input sentence (toy example)\n",
        "sentence = [\"What\", \"are\", \"the\", \"symptoms\", \"of\", \"diabetes\", \"?\"]\n",
        "vocab_size = 10000  # Assume a vocabulary size\n",
        "embedding_dim = 8   # Small embedding size for illustration\n",
        "\n",
        "# Randomly initialized embeddings for each token\n",
        "torch.manual_seed(42)\n",
        "embeddings = torch.rand((len(sentence), embedding_dim))  # (seq_length, d_model)\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "def self_attention(embeddings):\n",
        "    d_k = embeddings.shape[1]  # Embedding dimension\n",
        "\n",
        "    # Create Q, K, V matrices as learned linear transformations\n",
        "    W_q = torch.rand((d_k, d_k))\n",
        "    W_k = torch.rand((d_k, d_k))\n",
        "    W_v = torch.rand((d_k, d_k))\n",
        "\n",
        "    Q = embeddings @ W_q\n",
        "    K = embeddings @ W_k\n",
        "    V = embeddings @ W_v\n",
        "\n",
        "    # Compute attention scores\n",
        "    scores = Q @ K.T / torch.sqrt(torch.tensor(d_k))  # Scaled dot product\n",
        "    attention_weights = F.softmax(scores, dim=-1)  # Softmax normalization\n",
        "\n",
        "    # Compute self-attention output\n",
        "    output = attention_weights @ V\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "# Compute self-attention\n",
        "attention_scores, attention_output = self_attention(embeddings)\n",
        "\n",
        "# Print results\n",
        "print(\"Attention Scores:\\n\", attention_scores)\n",
        "print(\"\\nSelf-Attention Output:\\n\", attention_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFPwYFoOgXxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}